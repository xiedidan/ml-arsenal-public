{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing numerical libraries...\n",
      "Importing standard libraries...\n",
      "Importing miscellaneous functions...\n",
      "Importing constants...\n",
      "Importing Neural Network dependencies...\n",
      "\tPyTorch\n",
      "\tKeras\n",
      "\tTensorFlow\n",
      "\tMetrics, Losses and LR Schedulers\n",
      "\tKaggle Metrics\n",
      "\tImage augmentations\n",
      "\tDatasets\n",
      "Importing external libraries...\n",
      "\tLovasz Losses (elu+1)\n",
      "\n",
      "Fixing random seed for reproducibility...\n",
      "\tSetting random seed to 35202.\n",
      "\n",
      "Setting CUDA environment...\n",
      "\ttorch.__version__              = 1.1.0\n",
      "\ttorch.version.cuda             = 9.0.176\n",
      "\ttorch.backends.cudnn.version() = 7501\n",
      "\tos['CUDA_VISIBLE_DEVICES']     = 0,1\n",
      "\ttorch.cuda.device_count()      = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from dependencies import *\n",
    "from settings import *\n",
    "from reproducibility import *\n",
    "from models.TGS_salt.Unet34_scSE_hyper import Unet_scSE_hyper as Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "FACTOR = 128\n",
    "ne = \"ne\"\n",
    "initial_checkpoint = None\n",
    "MODEL = \"ResNet34\"\n",
    "\n",
    "batch_size = 16\n",
    "n_acc = 64 / batch_size\n",
    "nfolds = 4\n",
    "\n",
    "data_root = '../../data/siim-pneumothorax'\n",
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_str(time, str):\n",
    "    #if str == 'min':\n",
    "    #\t    return str(round(float(time)/60,5))+\" min(s)\"\n",
    "    return round(time,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Instead of directly printing to stdout, copy it into a txt file\n",
    "class Logger():\n",
    "    def __init__(self,path=None, fold=FOLD):\n",
    "        super().__init__()\n",
    "        self.fold=str(fold)\n",
    "        self.file = None \n",
    "    def write(self, str):\n",
    "        print(str)\n",
    "        self.file = open(self.fold+\"_log.txt\",\"w\") \n",
    "        self.file.write(str)\n",
    "        self.file.close()\n",
    "    def stop():\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_augment(image,mask,index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "    image, mask = do_resize2(image, mask, SIZE, SIZE)\n",
    "    image, mask = do_center_pad_to_factor2(image, mask, factor = FACTOR)\n",
    "    return image,mask,index,cache\n",
    "\n",
    "def train_augment(image,mask,index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "         image, mask = do_horizontal_flip2(image, mask)\n",
    "         pass\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        c = np.random.choice(4)\n",
    "        if c==0:\n",
    "            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.2) #0.125\n",
    "\n",
    "        if c==1:\n",
    "            image, mask = do_horizontal_shear2( image, mask, dx=np.random.uniform(-0.07,0.07) )\n",
    "            pass\n",
    "\n",
    "        if c==2:\n",
    "            image, mask = do_shift_scale_rotate2( image, mask, dx=0, dy=0, scale=1, angle=np.random.uniform(0,15))  #10\n",
    "\n",
    "        if c==3:\n",
    "            image, mask = do_elastic_transform2(image, mask, grid=10, distort=np.random.uniform(0,0.15))#0.10\n",
    "            pass\n",
    "    if np.random.rand() < 0.5:\n",
    "        c = np.random.choice(3)\n",
    "        if c==0:\n",
    "            image = do_brightness_shift(image,np.random.uniform(-0.1,+0.1))\n",
    "        if c==1:\n",
    "            image = do_brightness_multiply(image,np.random.uniform(1-0.08,1+0.08))\n",
    "        if c==2:\n",
    "            image = do_gamma(image,np.random.uniform(1-0.08,1+0.08))\n",
    "        # if c==1:\n",
    "        #     image = do_invert_intensity(image)\n",
    "\n",
    "    image, mask = do_resize2(image, mask, SIZE, SIZE)\n",
    "    image, mask = do_center_pad_to_factor2(image, mask, factor = FACTOR)\n",
    "    return image,mask,index,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing random seed for reproducibility...\n",
      "\tSetting random seed to 35202.\n",
      "\n",
      "Setting CUDA environment...\n",
      "\ttorch.__version__              = 1.1.0\n",
      "\ttorch.version.cuda             = 9.0.176\n",
      "\ttorch.backends.cudnn.version() = 7501\n",
      "\tos['CUDA_VISIBLE_DEVICES']     = 0,1\n",
      "\ttorch.cuda.device_count()      = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def null_augment(image, label, index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "    return image, label, index, cache\n",
    "\n",
    "def null_collate(batch):\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    cache = []\n",
    "    input = []\n",
    "    truth = []\n",
    "    index = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        truth.append(batch[b][1])\n",
    "        index.append(batch[b][2])\n",
    "        cache.append(batch[b][3])\n",
    "    input = torch.from_numpy(np.array(input)).float().unsqueeze(1)\n",
    "\n",
    "    if truth[0]!=[]:\n",
    "        truth = torch.from_numpy(np.array(truth)).float().unsqueeze(1)\n",
    "\n",
    "    return input, truth, index, cache\n",
    "\n",
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, data_root, fold, width=1024, height=1024, phase='train', augment=null_augment, random_state=2019, nfolds=4):\n",
    "        self.data_root = data_root\n",
    "        self.fold = fold\n",
    "        self.height = width\n",
    "        self.width = height\n",
    "        self.phase = phase\n",
    "        self.image_dir = img_dir\n",
    "        self.augment = augment\n",
    "        \n",
    "        kf = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "        train_list = os.listdir(os.path.join(data_root, 'train_png'))\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.filenames = list(kf.split(list(range(len(train_list)))))[fold][0]\n",
    "        elif phase == 'val':\n",
    "            self.filenames = list(kf.split(list(range(len(train_list)))))[fold][1]\n",
    "        else: # test\n",
    "            self.filenames = os.listdir(os.path.join(data_root, 'test_png'))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.data_root, 'train_png/{}'.format(self.filenames[index]))\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "        \n",
    "        if self.phase != 'test':\n",
    "            mask_path = os.path.join(self.data_root, 'mask_png/{}'.format(self.filenames[index]))\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "        else: # test\n",
    "            mask = []\n",
    "        \n",
    "        return self.augment(img, mask, index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation( net, valid_loader ):\n",
    "\n",
    "    valid_num  = 0\n",
    "    valid_loss = np.zeros(3,np.float32)\n",
    "\n",
    "    predicts = []\n",
    "    truths   = []\n",
    "\n",
    "    for input, truth, index, cache in valid_loader:\n",
    "        input = input.cuda()\n",
    "        truth = truth.cuda()\n",
    "        with torch.no_grad():\n",
    "            logit = net(input) #data_parallel(net,input)\n",
    "            prob  = F.sigmoid(logit)\n",
    "            loss  = net.focal_loss(logit, truth, 1.0, 0.5, 0.25) + net.criterion(logit, truth)\n",
    "            dice  = net.metric(logit, truth)\n",
    "\n",
    "        batch_size = len(index)\n",
    "        valid_loss += batch_size*np.array(( loss.item(), dice.item(), 0))\n",
    "        valid_num += batch_size\n",
    "\n",
    "        prob  = prob [:,:,Y0:Y1, X0:X1]\n",
    "        truth = truth[:,:,Y0:Y1, X0:X1]\n",
    "        #prob  = F.avg_pool2d(prob,  kernel_size=2, stride=2)\n",
    "        #truth = F.avg_pool2d(truth, kernel_size=2, stride=2)\n",
    "        predicts.append(prob.data.cpu().numpy())\n",
    "        truths.append(truth.data.cpu().numpy())\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler))\n",
    "    valid_loss  = valid_loss/valid_num\n",
    "\n",
    "    #--------------------------------------------------------\n",
    "    predicts = np.concatenate(predicts).squeeze()\n",
    "    truths   = np.concatenate(truths).squeeze()\n",
    "    precision, result, threshold  = do_kaggle_metric(predicts, truths)\n",
    "    valid_loss[2] = precision.mean()\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(net):\n",
    "    for p in net.feature_net.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "def unfreeze(net):\n",
    "    for p in net.feature_net.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def cosine_annealing_scheduler(num_iter, lr_init, lr_min):\n",
    "    scheduler = lambda x: ((lr_init-lr_min)/2)*(np.cos(PI*(np.mod(x-1,num_iter)/(num_iter)))+1)+lr_min\n",
    "        \n",
    "def set_BN_momentum(model, momentum=0.1*batch_size/64):\n",
    "    for i, (name, layer) in enumerate(model.named_modules()):\n",
    "        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.momentum = momentum\n",
    "            \n",
    "def fit_one_cycle(epochs, net, train_loader, val_loader, lr_init=0.001, lr_min=0.00001):\n",
    "    # init learner\n",
    "    num_batch = len(train_loader)\n",
    "    num_iter = num_batch * epochs\n",
    "    iter_smooth = 20\n",
    "    iter_log    = 100\n",
    "    iter_valid  = num_batch\n",
    "    \n",
    "    scheduler = cosine_annealing_scheduler(num_iter, lr_init, lr_min)\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "          lr=0.01, momentum=0.9, weight_decay=0.0001\n",
    "    )\n",
    "    set_BN_momentum(net)\n",
    "    \n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    train_loss  = np.zeros(6,np.float32)\n",
    "    valid_loss  = np.zeros(6,np.float32)\n",
    "    batch_loss  = np.zeros(6,np.float32)\n",
    "    rate = 0\n",
    "    iter = 0\n",
    "    \n",
    "    start = timer()\n",
    "    while iter < num_iters:  # loop over the dataset multiple times\n",
    "        sum_train_loss = np.zeros(6,np.float32)\n",
    "        sum = 0\n",
    "            \n",
    "        for input, truth, index, cache in train_loader:\n",
    "            # validation\n",
    "            if (iter + 1) % iter_valid == 0:\n",
    "                net.set_mode('valid')\n",
    "                valid_loss = validation(net, valid_loader)\n",
    "                net.set_mode('train')\n",
    "\n",
    "                log.write2('\\r')\n",
    "                log.write('%0.4f  %5.1f  %6.1f  |  %0.3f  %0.3f  (%0.3f) |  %0.3f  %0.3f  |  %0.3f  %0.3f  | %s \\n' % (\\\n",
    "                         rate, iter/1000, epoch,\n",
    "                         valid_loss[0], valid_loss[1], valid_loss[2],\n",
    "                         train_loss[0], train_loss[1],\n",
    "                         batch_loss[0], batch_loss[1],\n",
    "                         time_to_str((timer() - start),'min')))\n",
    "                time.sleep(0.01)\n",
    "                \n",
    "            if scheduler is not None:\n",
    "                lr = scheduler(iter)\n",
    "                if lr<0 : break\n",
    "                adjust_learning_rate(optimizer, lr)\n",
    "                rate = get_learning_rate(optimizer)\n",
    "            \n",
    "            # ok, train\n",
    "            net.set_mode('train')\n",
    "\n",
    "            input = input.cuda()\n",
    "            truth = truth.cuda()\n",
    "\n",
    "            logit = net(input) #data_parallel(net,input)\n",
    "\n",
    "            if OHEM == \"OHEM\":\n",
    "                loss = net.focal_loss(logit, truth, 1.0, 0.5, 0.25) + net.criterion(logit, truth)\n",
    "            else:\n",
    "                loss = net.criterion(logit, truth)\n",
    "\n",
    "            dice = net.metric(logit, truth)\n",
    "            \n",
    "            # learn with grad acc\n",
    "            loss /= n_acc\n",
    "            loss.backward()\n",
    "            \n",
    "            if ((iter + 1) % n_acc) == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.nn.utils.clip_grad_norm(net.parameters(), 1)\n",
    "            \n",
    "            # print statistics  ------------\n",
    "            batch_loss = np.array((\n",
    "                           loss.item(),\n",
    "                           dice.item(),\n",
    "                           0, 0, 0, 0,\n",
    "                         ))\n",
    "            sum_train_loss += batch_loss\n",
    "            sum += 1\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/sum\n",
    "                sum_train_loss = np.zeros(6,np.float32)\n",
    "                sum = 0\n",
    "\n",
    "            log.write2('\\r%0.4f  %5.1f  %6.1f  |  %0.3f  %0.3f  (%0.3f) |  %0.3f  %0.3f  |  %0.3f  %0.3f  | %s ' % (\\\n",
    "                         rate, iter/1000, epoch,\n",
    "                         valid_loss[0], valid_loss[1], valid_loss[2],\n",
    "                         train_loss[0], train_loss[1],\n",
    "                         batch_loss[0], batch_loss[1],\n",
    "                         time_to_str((timer() - start), 'min')))\n",
    "            \n",
    "            iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_root, batch_size, fold, nfolds=4, width=1024, height=1024, train_augment=null_augment, val_augment=null_augment, random_state=SEED):\n",
    "    train_dataset = SIIMDataset(\n",
    "        data_root,\n",
    "        fold,\n",
    "        width=width, height=height,\n",
    "        phase='train',\n",
    "        augment=train_augment,\n",
    "        random_state=random_state,\n",
    "        nfolds=nfolds\n",
    "    )\n",
    "\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler     = RandomSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    valid_dataset = SIIMDataset(\n",
    "        data_root,\n",
    "        fold,\n",
    "        width=width, height=height,\n",
    "        phase='val',\n",
    "        augment=val_augment,\n",
    "        random_state=random_state,\n",
    "        nfolds=nfolds\n",
    "    )\n",
    "\n",
    "    valid_loader  = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler     = RandomSampler(valid_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = False,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training U-Net with hypercolumn concatenation and spatial/channel-wise excitation...\")\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "for fold in range(nfolds):\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        data_root,\n",
    "        batch_size,\n",
    "        fold, nfolds=nfolds,\n",
    "        width=SIZE, height=SIZE,\n",
    "        train_augment=null_augment, val_augment=null_augment,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    net = Net().cuda()\n",
    "    \n",
    "    lr = 0.001\n",
    "    \n",
    "    # warm up\n",
    "    freeze(net)\n",
    "    fit_one_cycle(\n",
    "        6, net,\n",
    "        train_loader, val_loader,\n",
    "        lr_init=lr, lr_min=lr/100\n",
    "    )\n",
    "    \n",
    "    # ok, train\n",
    "    unfreeze(net)\n",
    "    fit_one_cycle(\n",
    "        12, net,\n",
    "        train_loader, val_loader,\n",
    "        lr_init=lr/2, lr_min=lr/80\n",
    "    )\n",
    "    \n",
    "    # save\n",
    "    torch.save(net.state_dict(), './model/unet_{}_fold{}.pth'.format())\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tgs)",
   "language": "python",
   "name": "tgs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
