{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BESNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using paths on kail-main\n",
      "\n",
      "Importing numerical libraries...\n",
      "Importing standard libraries...\n",
      "Importing miscellaneous functions...\n",
      "Importing constants...\n",
      "Importing Neural Network dependencies...\n",
      "\tPyTorch\n",
      "\tKeras\n",
      "\tTensorFlow\n",
      "\tMetrics, Losses and LR Schedulers\n",
      "\tKaggle Metrics\n",
      "\tImage augmentations\n",
      "\tDatasets\n",
      "Importing external libraries...\n",
      "\tLovasz Losses (elu+1)\n",
      "\n",
      "Fixing random seed for reproducibility...\n",
      "\tSetting random seed to 35202.\n",
      "\n",
      "Setting CUDA environment...\n",
      "\ttorch.__version__              = 1.1.0\n",
      "\ttorch.version.cuda             = 9.0.176\n",
      "\ttorch.backends.cudnn.version() = 7501\n",
      "\tos['CUDA_VISIBLE_DEVICES']     = 0,1\n",
      "\ttorch.cuda.device_count()      = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dependencies import *\n",
    "from settings import *\n",
    "from reproducibility import *\n",
    "from models.TGS_salt.BesNet import BesNet as Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "FACTOR = SIZE\n",
    "ne = \"ne\"\n",
    "initial_checkpoint = None\n",
    "MODEL = \"ResNet34\"\n",
    "\n",
    "batch_size = 8\n",
    "n_acc = 256 / batch_size\n",
    "nfolds = 4\n",
    "\n",
    "noise_th = 75.0*(SIZE/128.0)**2 #threshold for the number of predicted pixels\n",
    "best_thr0 = 0.2 #preliminary value of the threshold for metric calculation\n",
    "\n",
    "data_root = '../../data/siim-pneumothorax'\n",
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_str(time, str):\n",
    "    #if str == 'min':\n",
    "    #\t    return str(round(float(time)/60,5))+\" min(s)\"\n",
    "    return round(time,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Instead of directly printing to stdout, copy it into a txt file\n",
    "class Logger():\n",
    "    def __init__(self,name=MODEL+ne):\n",
    "        super().__init__()\n",
    "        self.model=name\n",
    "        #if OHEM != \"OHEM\":\n",
    "        #    self.model=MODEL+ne[ne.find(\"_\")+1:]\n",
    "        self.file = open(self.model+\"_bes_log.txt\",\"w+\")\n",
    "        self.file.close()\n",
    "        \n",
    "        self.debug_file = open(self.model + '_bes_debug.txt', 'w+')\n",
    "        self.debug_file.close()\n",
    "    def write(self, str):\n",
    "        print(str)\n",
    "        self.file = open(self.model+\"_bes_log.txt\",\"a+\")\n",
    "        self.file.write(str)\n",
    "        self.file.close()\n",
    "    def write2(self, str):\n",
    "        print(str, end='',flush=True)\n",
    "        self.file = open(self.model+\"_bes_log.txt\",\"a+\")\n",
    "        self.file.write(str)\n",
    "        self.file.close()\n",
    "    def debug(self, str):\n",
    "        self.debug_file = open(self.model + '_bes_debug.txt', 'a+')\n",
    "        self.debug_file.write(str)\n",
    "        self.debug_file.close()\n",
    "    def stop():\n",
    "        self.file.close()\n",
    "        self.debug_file.close()\n",
    "        \n",
    "log = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_augment(image,mask,index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "    # image, mask = do_resize2(image, mask, SIZE, SIZE)\n",
    "    # image, mask = do_center_pad_to_factor2(image, mask, factor = FACTOR)\n",
    "    return image,mask,index,cache\n",
    "\n",
    "def train_augment(image,mask,index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "         image, mask = do_horizontal_flip2(image, mask)\n",
    "         pass\n",
    "\n",
    "    if np.random.rand() < 0.2:\n",
    "        c = np.random.choice(4)\n",
    "        if c==0:\n",
    "            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.1) #0.125\n",
    "\n",
    "        if c==1:\n",
    "            image, mask = do_horizontal_shear2( image, mask, dx=np.random.uniform(-0.02,0.02) )\n",
    "            pass\n",
    "\n",
    "        if c==2:\n",
    "            image, mask = do_shift_scale_rotate2( image, mask, dx=0, dy=0, scale=1, angle=np.random.uniform(0,15))  #10\n",
    "\n",
    "        if c==3:\n",
    "            image, mask = do_elastic_transform2(image, mask, grid=10, distort=np.random.uniform(0,0.05))#0.10\n",
    "            pass\n",
    "    if np.random.rand() < 0.1:\n",
    "        c = np.random.choice(3)\n",
    "        if c==0:\n",
    "            image = do_brightness_shift(image,np.random.uniform(-0.1,+0.1))\n",
    "        if c==1:\n",
    "            image = do_brightness_multiply(image,np.random.uniform(1-0.08,1+0.08))\n",
    "        if c==2:\n",
    "            image = do_gamma(image,np.random.uniform(1-0.08,1+0.08))\n",
    "        # if c==1:\n",
    "        #     image = do_invert_intensity(image)\n",
    "\n",
    "    # image, mask = do_resize2(image, mask, SIZE, SIZE)\n",
    "    # image, mask = do_center_pad_to_factor2(image, mask, factor = FACTOR)\n",
    "    return image,mask,index,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_augment(image, mask, index):\n",
    "    cache = Struct(image = image.copy(), mask = mask.copy())\n",
    "    return image, mask, index, cache\n",
    "\n",
    "def null_collate(batch):\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    cache = []\n",
    "    input = []\n",
    "    truth = []\n",
    "    index = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        truth.append(batch[b][1])\n",
    "        index.append(batch[b][2])\n",
    "        cache.append(batch[b][3])\n",
    "    input = torch.from_numpy(np.array(input)).float().unsqueeze(1)\n",
    "\n",
    "    if truth[0]!=[]:\n",
    "        truth = torch.from_numpy(np.array(truth)).float().unsqueeze(1)\n",
    "\n",
    "    return input, truth, index, cache\n",
    "\n",
    "def get_weights_for_balanced_classes(cls_list, num_classes):\n",
    "    # get count per class\n",
    "    count = [0] * num_classes\n",
    "    \n",
    "    for cls in cls_list:\n",
    "        count[cls] += 1\n",
    "\n",
    "    # get weight per class\n",
    "    weight_per_class = [0.] * num_classes\n",
    "    N = float(len(cls_list))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        weight_per_class[i] = N / float(count[i])\n",
    "        \n",
    "    #ã€€get weight per sample\n",
    "    weights = [0] * len(cls_list)\n",
    "    \n",
    "    for i, cls in enumerate(cls_list):\n",
    "        weights[i] = weight_per_class[cls]\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def get_boundary(masks):\n",
    "    mask_arr = (masks.cpu().numpy() * 255).astype(np.uint8).squeeze()\n",
    "    b_arr = []\n",
    "    \n",
    "    for mask in mask_arr:\n",
    "        b_img = np.zeros(mask.shape)\n",
    "        \n",
    "        contours, hier = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        cv2.drawContours(b_img, contours, -1, 255, 1)\n",
    "        \n",
    "        b_arr.append(b_img)\n",
    "        \n",
    "    b_arr = np.stack(b_arr)\n",
    "    \n",
    "    return torch.from_numpy(b_arr)\n",
    "\n",
    "class SIIMDataset(Dataset):\n",
    "    def __init__(self, data_root, fold, pos_neg_ratio=0.5, width=1024, height=1024, phase='train', augment=null_augment, random_state=2019, nfolds=4):\n",
    "        self.data_root = data_root\n",
    "        self.fold = fold\n",
    "        self.height = width\n",
    "        self.width = height\n",
    "        self.phase = phase\n",
    "        self.augment = augment\n",
    "        \n",
    "        kf = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "        train_list = os.listdir(os.path.join(data_root, 'train_png'))\n",
    "        \n",
    "        if phase == 'train':\n",
    "            index_list = list(kf.split(list(range(len(train_list)))))[fold][0]\n",
    "            self.filenames = [train_list[i] for i in index_list]\n",
    "            \n",
    "            # read masks for pos/neg ratio sampler\n",
    "            train_df = pd.read_csv(os.path.join(self.data_root, 'train-rle.csv'))\n",
    "            pos_ids = list(train_df[train_df[' EncodedPixels']!=' -1']['ImageId'])\n",
    "\n",
    "            self.cls_list = [1 if filename.split('.png')[0] in pos_ids else 0 for filename in self.filenames]\n",
    "\n",
    "        elif phase == 'val':\n",
    "            index_list = list(kf.split(list(range(len(train_list)))))[fold][1]\n",
    "            self.filenames = [train_list[i] for i in index_list]\n",
    "        else: # test\n",
    "            self.filenames = os.listdir(os.path.join(data_root, 'test_png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.data_root, 'train_png/{}'.format(self.filenames[index]))\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n",
    "        img = cv2.resize(img, (self.width, self.height), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        if self.phase == 'test':\n",
    "            mask = []\n",
    "        else: # train and val\n",
    "            mask_path = os.path.join(self.data_root, 'mask_png/{}'.format(self.filenames[index]))\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n",
    "            mask = cv2.resize(mask, (self.width, self.height), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        return self.augment(img, mask, index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation( net, valid_loader, weights=None ):\n",
    "\n",
    "    valid_num  = 0\n",
    "    valid_loss = np.zeros(3, np.float32)\n",
    "    \n",
    "    logits = []\n",
    "    truths = []\n",
    "    for input, truth, index, cache in valid_loader:\n",
    "        input = input.cuda()\n",
    "        truth = truth.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            b_masks = get_boundary(truth).float().cuda() / 255.\n",
    "            \n",
    "            m_logit, b_logit = net(input) #data_parallel(net,input)\n",
    "            \n",
    "            b_loss = net.boundary_criterion(b_logit, b_masks, weights=weights)\n",
    "            m_loss = net.mask_criterion(m_logit, b_logit, truth, b_masks, alpha=5., beta=0.2, weights=weights)\n",
    "            loss = b_loss + m_loss\n",
    "            loss = loss.sum()\n",
    "            \n",
    "            dice  = net.metric(m_logit, truth, noise_th=noise_th, threshold=best_thr0, logger=log)\n",
    "            \n",
    "            logits.append(m_logit.cpu())\n",
    "            truths.append(truth.cpu())\n",
    "\n",
    "        batch_size = len(index)\n",
    "        valid_loss += batch_size * np.array(( loss.item(), dice.item(), 0))\n",
    "        valid_num += batch_size\n",
    "        \n",
    "    valid_loss /= valid_num\n",
    "    \n",
    "    # find out optimal thr and dice\n",
    "    log.debug('\\nscan\\n')\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    truths = torch.cat(truths, dim=0)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    thrs = np.arange(0.05, 1, 0.05)\n",
    "    \n",
    "    th_dices = []\n",
    "    for th in thrs:\n",
    "        th_dice = net.metric(logits, truths, noise_th=noise_th, threshold=th, logger=log)\n",
    "        th_dices.append(th_dice)\n",
    "        \n",
    "    th_dices = np.array(th_dices)\n",
    "    best_dice = th_dices.max()\n",
    "    best_thr = thrs[th_dices.argmax()]\n",
    "    \n",
    "    valid_loss[1] = best_dice\n",
    "    valid_loss[2] = best_thr\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(net):\n",
    "    for p in net.conv1.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.encoder2.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.encoder3.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.encoder4.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.encoder5.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.center.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.decoder5.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.decoder4.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.decoder3.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.decoder2.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for p in net.decoder1.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "def unfreeze(net):\n",
    "    for p in net.conv1.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.encoder2.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.encoder3.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.encoder4.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.encoder5.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.center.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.decoder5.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.decoder4.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.decoder3.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.decoder2.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    for p in net.decoder1.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def cosine_annealing_scheduler(num_iter, lr_init, lr_min):\n",
    "    scheduler = lambda x: ((lr_init-lr_min)/2)*(np.cos(PI*(np.mod(x,num_iter)/(num_iter)))+1)+lr_min\n",
    "    return scheduler\n",
    "        \n",
    "def set_BN_momentum(model, momentum=0.1*batch_size/64):\n",
    "    for i, (name, layer) in enumerate(model.named_modules()):\n",
    "        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.momentum = momentum\n",
    "            \n",
    "def fit_one_cycle(epochs, net, train_loader, val_loader, lr_init=0.001, lr_min=0.000001, weights=None):\n",
    "    # init learner\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    num_iter = iter_per_epoch * epochs\n",
    "    iter_smooth = 20\n",
    "    iter_log    = 100\n",
    "    iter_valid  = iter_per_epoch\n",
    "    #iter_valid = 100\n",
    "    \n",
    "    #scheduler = None\n",
    "    scheduler = cosine_annealing_scheduler(num_iter, lr_init, lr_min)\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "          lr=lr_init, momentum=0.9, weight_decay=0.0001\n",
    "    )\n",
    "    set_BN_momentum(net)\n",
    "    \n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    train_loss  = np.zeros(6,np.float32)\n",
    "    valid_loss  = np.zeros(6,np.float32)\n",
    "    batch_loss  = np.zeros(6,np.float32)\n",
    "    rate = 0\n",
    "    iter = 0\n",
    "    epoch = 0\n",
    "    \n",
    "    #debug\n",
    "    if 0: #debug  ##-------------------------------\n",
    "        debug_num = 2\n",
    "        debug_count = 0\n",
    "        \n",
    "        for input, truth, index, cache in train_loader:\n",
    "            images = input.cpu().data.numpy().squeeze()\n",
    "            masks  = truth.cpu().data.numpy().squeeze()\n",
    "            \n",
    "            batch_size = len(index)\n",
    "            for b in range(batch_size):\n",
    "                image = images[b]*255\n",
    "                image = np.dstack([image,image,image])\n",
    "\n",
    "                mask = masks[b]\n",
    "                print(np.max(mask))\n",
    "                \n",
    "                # Plot some samples\n",
    "                fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "                ax0.imshow(image.astype(np.uint8))\n",
    "                ax1.imshow(mask, vmin=0, vmax=1)\n",
    "                ax1.set_title('Targets')\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "            debug_count += 1\n",
    "            if debug_count > debug_num:\n",
    "                break\n",
    "    #--------------------------------------\n",
    "    \n",
    "    start = timer()\n",
    "    while iter < num_iter:  # loop over the dataset multiple times\n",
    "        sum_train_loss = np.zeros(6,np.float32)\n",
    "        sum = 0\n",
    "\n",
    "        log.write('\\n rate    iter   epoch   | valid_loss               | train_loss               | batch_loss               |  time          \\n')\n",
    "        log.write('-------------------------------------------------------------------------------------------------------------------------------\\n')\n",
    "            \n",
    "        for input, truth, index, cache in train_loader:\n",
    "            # validation\n",
    "            if (iter + 1) % iter_valid == 0:\n",
    "                log.debug('\\nval\\n')\n",
    "                net.set_mode('valid')\n",
    "                valid_loss = validation(net, val_loader)\n",
    "\n",
    "                net.set_mode('train')\n",
    "                log.debug('\\ntrain\\n')\n",
    "                time.sleep(0.01)\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                lr = scheduler(iter)\n",
    "                if lr<0 : break\n",
    "                adjust_learning_rate(optimizer, lr)\n",
    "                rate = get_learning_rate(optimizer)\n",
    "            \n",
    "            # ok, train\n",
    "            net.set_mode('train')\n",
    "\n",
    "            input = input.cuda().float()\n",
    "            truth = truth.cuda().float()\n",
    "            \n",
    "            b_masks = (get_boundary(truth).float().cuda() / 255.).float()\n",
    "\n",
    "            m_logit, b_logit = net(input) #data_parallel(net,input)\n",
    "\n",
    "            b_loss = net.boundary_criterion(b_logit, b_masks, weights=weights)\n",
    "            m_loss = net.mask_criterion(m_logit, b_logit.detach(), truth, b_masks.detach(), alpha=5., beta=0.2, weights=weights)\n",
    "            #print('\\nb_loss: {}, m_loss: {}'.format(b_loss.mean(), m_loss.mean()))\n",
    "            loss = (b_loss + m_loss).sum()\n",
    "                \n",
    "            dice = net.metric(m_logit, truth, noise_th=noise_th, threshold=best_thr0, logger=log)\n",
    "            \n",
    "            # learn with grad acc\n",
    "            loss /= n_acc\n",
    "            loss.backward()\n",
    "            \n",
    "            if ((iter + 1) % n_acc) == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            \n",
    "            # print statistics  ------------\n",
    "            batch_loss = np.array((\n",
    "                           loss.item(),\n",
    "                           dice.item(),\n",
    "                           0, 0, 0, 0,\n",
    "                         ))\n",
    "            sum_train_loss += batch_loss\n",
    "            sum += 1\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/sum\n",
    "                sum_train_loss = np.zeros(6,np.float32)\n",
    "                sum = 0\n",
    "\n",
    "            log.write2('\\r%0.4f  %5.1f  %6.1f  |  %0.3f  %0.3f  (%0.3f) |  %0.3f  %0.3f  |  %0.3f  %0.3f  | %s ' % (\\\n",
    "                         rate, iter/iter_per_epoch, epoch+1,\n",
    "                         valid_loss[0], valid_loss[1], valid_loss[2],\n",
    "                         train_loss[0], train_loss[1],\n",
    "                         batch_loss[0], batch_loss[1],\n",
    "                         time_to_str((timer() - start), 'min')))\n",
    "            \n",
    "            iter += 1\n",
    "            epoch = iter // iter_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_root, batch_size, fold, nfolds=4, width=1024, height=1024, train_augment=null_augment, val_augment=null_augment, random_state=SEED):\n",
    "    train_dataset = SIIMDataset(\n",
    "        data_root,\n",
    "        fold,\n",
    "        width=width, height=height,\n",
    "        phase='train',\n",
    "        augment=train_augment,\n",
    "        random_state=random_state,\n",
    "        nfolds=nfolds\n",
    "    )\n",
    "    \n",
    "    weights = get_weights_for_balanced_classes(train_dataset.cls_list, 2)\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    balance_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        # sampler     = RandomSampler(train_dataset),\n",
    "        sampler = balance_sampler,\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    val_dataset = SIIMDataset(\n",
    "        data_root,\n",
    "        fold,\n",
    "        width=width, height=height,\n",
    "        phase='val',\n",
    "        augment=val_augment,\n",
    "        random_state=random_state,\n",
    "        nfolds=nfolds\n",
    "    )\n",
    "\n",
    "    val_loader  = DataLoader(\n",
    "        val_dataset,\n",
    "        sampler     = RandomSampler(val_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = False,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one fold test!\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_root,\n",
    "    batch_size,\n",
    "    0, nfolds=10,\n",
    "    width=SIZE, height=SIZE,\n",
    "    train_augment=train_augment, val_augment=valid_augment,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "net = Net().cuda()\n",
    "\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " rate    iter   epoch   | valid_loss               | train_loss               | batch_loss               |  time          \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001    1.0     1.0  |  0.000  0.000  (0.000) |  0.056  0.011  |  0.054  0.008  | 227.0242 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001    1.0     1.0  |  0.817  0.005  (0.300) |  0.056  0.011  |  0.071  0.005  | 251.7494 \n",
      " rate    iter   epoch   | valid_loss               | train_loss               | batch_loss               |  time          \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000    2.0     2.0  |  0.817  0.005  (0.300) |  0.052  0.013  |  0.052  0.009  | 478.5055 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000    2.0     2.0  |  0.802  0.005  (0.300) |  0.052  0.013  |  0.053  0.012  | 503.5293 "
     ]
    }
   ],
   "source": [
    "# warm up\n",
    "freeze(net)\n",
    "fit_one_cycle(\n",
    "    2, net,\n",
    "    train_loader, val_loader,\n",
    "    lr_init=lr, lr_min=lr/100,\n",
    "    weights=[0.25, 0.75]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'bes-cp-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('bes-cp-1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " rate    iter   epoch   | valid_loss               | train_loss               | batch_loss               |  time          \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000    0.7     1.0  |  0.000  0.000  (0.000) |  0.047  0.024  |  0.048  0.016  | 328.32 "
     ]
    }
   ],
   "source": [
    "# ok, train positive\n",
    "unfreeze(net)\n",
    "fit_one_cycle(\n",
    "    50, net,\n",
    "    train_loader, val_loader,\n",
    "    lr_init=lr/2, lr_min=lr/120,\n",
    "    weights=[0.25, 0.75]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'bes-cp-2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(\n",
    "    50, net,\n",
    "    train_loader, val_loader,\n",
    "    lr_init=lr/20, lr_min=lr/250,\n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'bes-cp-3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('bes-cp-3.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloaders(\n",
    "    data_root,\n",
    "    batch_size,\n",
    "    0, nfolds=10,\n",
    "    width=SIZE, height=SIZE,\n",
    "    train_augment=train_augment, val_augment=valid_augment,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('bes-cp-3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA with flip-lr\n",
    "\n",
    "def tta(net, loader):\n",
    "    net.set_mode('test')\n",
    "    \n",
    "    all_probs = []\n",
    "    all_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader), file=sys.stdout) as pbar:\n",
    "            for input, truth, index, cache in loader:\n",
    "                input = input.cuda()\n",
    "\n",
    "                logits = net(input)\n",
    "                probs = F.sigmoid(logits)\n",
    "\n",
    "                all_probs.append(probs)\n",
    "                all_truths.append(truth)\n",
    "\n",
    "                pbar.update(1)\n",
    "            \n",
    "        i = 0\n",
    "        with tqdm(total=len(loader), file=sys.stdout) as pbar:\n",
    "            # tensor.shape = [N, C, H, W], so we flip dim -1\n",
    "            for input, truth, index, cache in loader:\n",
    "                input = input.cuda()\n",
    "                input = torch.flip(input, [-1])\n",
    "\n",
    "                logits = net(input)\n",
    "                probs = F.sigmoid(logits)\n",
    "\n",
    "                probs = torch.flip(probs, [-1])\n",
    "\n",
    "                all_probs[i] += probs\n",
    "                all_probs[i] *= 0.5\n",
    "\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_truths = torch.cat(all_truths, dim=0)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_probs, all_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fake tta!\n",
    "\n",
    "def fake_tta(net, loader):\n",
    "    net.set_mode('test')\n",
    "    \n",
    "    all_m_probs = []\n",
    "    all_b_probs = []\n",
    "    all_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(loader), file=sys.stdout) as pbar:\n",
    "            for input, truth, index, cache in loader:\n",
    "                input = input.cuda()\n",
    "\n",
    "                m_logits, b_logits = net(input)\n",
    "                m_probs = F.sigmoid(m_logits)\n",
    "                b_probs = F.sigmoid(b_logits)\n",
    "\n",
    "                all_m_probs.append(m_probs)\n",
    "                all_b_probs.append(b_probs)\n",
    "                all_truths.append(truth)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if 0: #debug  ##-------------------------------\n",
    "                    images = input.cpu().detach().numpy().squeeze()\n",
    "                    masks  = truth.cpu().detach().numpy().squeeze()\n",
    "                    \n",
    "                    m_results = m_probs.cpu().detach().numpy().squeeze()\n",
    "                    b_results = b_probs.cpu().detach().numpy().squeeze()\n",
    "                    \n",
    "                    batch_size = len(index)\n",
    "                    \n",
    "                    for b in range(batch_size):\n",
    "                        image = images[b]*255\n",
    "                        image = np.dstack([image,image,image])\n",
    "\n",
    "                        mask = masks[b]\n",
    "                        m_result = m_results[b]\n",
    "                        b_result = b_results[b]\n",
    "                        \n",
    "                        # Plot some samples\n",
    "                        fig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "                        \n",
    "                        ax0.imshow(image.astype(np.uint8))\n",
    "                        \n",
    "                        ax1.imshow(mask, vmin=0, vmax=1)\n",
    "                        ax1.set_title('truth')\n",
    "                        \n",
    "                        ax2.imshow(m_result, vmin=0, vmax=1)\n",
    "                        ax2.set_title('m_prob')\n",
    "                        \n",
    "                        ax3.imshow(b_result, vmin=0, vmax=1)\n",
    "                        ax3.set_title('b_prob')\n",
    "\n",
    "                        plt.show()\n",
    "                #--------------------------------------\n",
    "\n",
    "        all_m_probs = torch.cat(all_m_probs, dim=0)\n",
    "        all_b_probs = torch.cat(all_b_probs, dim=0)\n",
    "        all_truths = torch.cat(all_truths, dim=0)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_m_probs, all_b_probs, all_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dice for threshold selection\n",
    "\n",
    "def dice_overall(preds, targs):\n",
    "    n = preds.shape[0]\n",
    "    \n",
    "    preds = preds.view(n, -1)\n",
    "    targs = targs.view(n, -1)\n",
    "    \n",
    "    targs = (targs > 0.5).long()\n",
    "    \n",
    "    intersect = (preds * targs).sum(-1).float()\n",
    "    union = (preds + targs).sum(-1).float()\n",
    "    \n",
    "    # get 1 for both empty pred and targ\n",
    "    u0 = union==0\n",
    "    intersect[u0] = 1\n",
    "    union[u0] = 2\n",
    "    \n",
    "    return (2. * intersect / union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/134 [00:00<01:05,  2.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/home/voyager/anaconda3/envs/tgs/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134/134 [00:15<00:00,  8.68it/s]\n",
      "tensor([ 68.6168, 188.8050,  76.6630,  ...,  37.3683, 405.8776,  53.5727],\n",
      "       device='cuda:0')\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:01<00:00, 58.47it/s]\n",
      "[tensor(0.6575, device='cuda:0') tensor(0.6616, device='cuda:0')\n",
      " tensor(0.6641, device='cuda:0') tensor(0.6658, device='cuda:0')\n",
      " tensor(0.6672, device='cuda:0') tensor(0.6682, device='cuda:0')\n",
      " tensor(0.6692, device='cuda:0') tensor(0.6700, device='cuda:0')\n",
      " tensor(0.6707, device='cuda:0') tensor(0.6714, device='cuda:0')\n",
      " tensor(0.6720, device='cuda:0') tensor(0.6725, device='cuda:0')\n",
      " tensor(0.6730, device='cuda:0') tensor(0.6735, device='cuda:0')\n",
      " tensor(0.6740, device='cuda:0') tensor(0.6744, device='cuda:0')\n",
      " tensor(0.6748, device='cuda:0') tensor(0.6752, device='cuda:0')\n",
      " tensor(0.6755, device='cuda:0') tensor(0.6759, device='cuda:0')\n",
      " tensor(0.6762, device='cuda:0') tensor(0.6765, device='cuda:0')\n",
      " tensor(0.6768, device='cuda:0') tensor(0.6771, device='cuda:0')\n",
      " tensor(0.6774, device='cuda:0') tensor(0.6777, device='cuda:0')\n",
      " tensor(0.6780, device='cuda:0') tensor(0.6782, device='cuda:0')\n",
      " tensor(0.6785, device='cuda:0') tensor(0.6788, device='cuda:0')\n",
      " tensor(0.6790, device='cuda:0') tensor(0.6793, device='cuda:0')\n",
      " tensor(0.6795, device='cuda:0') tensor(0.6797, device='cuda:0')\n",
      " tensor(0.6800, device='cuda:0') tensor(0.6802, device='cuda:0')\n",
      " tensor(0.6804, device='cuda:0') tensor(0.6806, device='cuda:0')\n",
      " tensor(0.6809, device='cuda:0') tensor(0.6811, device='cuda:0')\n",
      " tensor(0.6813, device='cuda:0') tensor(0.6815, device='cuda:0')\n",
      " tensor(0.6817, device='cuda:0') tensor(0.6819, device='cuda:0')\n",
      " tensor(0.6821, device='cuda:0') tensor(0.6823, device='cuda:0')\n",
      " tensor(0.6825, device='cuda:0') tensor(0.6826, device='cuda:0')\n",
      " tensor(0.6828, device='cuda:0') tensor(0.6830, device='cuda:0')\n",
      " tensor(0.6832, device='cuda:0') tensor(0.6833, device='cuda:0')\n",
      " tensor(0.6835, device='cuda:0') tensor(0.6837, device='cuda:0')\n",
      " tensor(0.6838, device='cuda:0') tensor(0.6840, device='cuda:0')\n",
      " tensor(0.6841, device='cuda:0') tensor(0.6843, device='cuda:0')\n",
      " tensor(0.6844, device='cuda:0') tensor(0.6845, device='cuda:0')\n",
      " tensor(0.6847, device='cuda:0') tensor(0.6848, device='cuda:0')\n",
      " tensor(0.6848, device='cuda:0') tensor(0.6850, device='cuda:0')\n",
      " tensor(0.6851, device='cuda:0') tensor(0.6851, device='cuda:0')\n",
      " tensor(0.6853, device='cuda:0') tensor(0.6862, device='cuda:0')\n",
      " tensor(0.6864, device='cuda:0') tensor(0.6865, device='cuda:0')\n",
      " tensor(0.6874, device='cuda:0') tensor(0.6875, device='cuda:0')\n",
      " tensor(0.6875, device='cuda:0') tensor(0.6875, device='cuda:0')\n",
      " tensor(0.6874, device='cuda:0') tensor(0.6873, device='cuda:0')\n",
      " tensor(0.6873, device='cuda:0') tensor(0.6881, device='cuda:0')\n",
      " tensor(0.6880, device='cuda:0') tensor(0.6878, device='cuda:0')\n",
      " tensor(0.6876, device='cuda:0') tensor(0.6901, device='cuda:0')\n",
      " tensor(0.6918, device='cuda:0') tensor(0.6952, device='cuda:0')\n",
      " tensor(0.6985, device='cuda:0') tensor(0.6999, device='cuda:0')\n",
      " tensor(0.7022, device='cuda:0') tensor(0.7061, device='cuda:0')\n",
      " tensor(0.7091, device='cuda:0') tensor(0.7139, device='cuda:0')\n",
      " tensor(0.7204, device='cuda:0') tensor(0.7285, device='cuda:0')\n",
      " tensor(0.7346, device='cuda:0') tensor(0.7402, device='cuda:0')\n",
      " tensor(0.7470, device='cuda:0') tensor(0.7597, device='cuda:0')\n",
      " tensor(0.7704, device='cuda:0') tensor(0.7764, device='cuda:0')\n",
      " tensor(0.7783, device='cuda:0')] [tensor(0.7783, device='cuda:0')] [0.99]\n"
     ]
    }
   ],
   "source": [
    "m_probs, b_probs, truths = fake_tta(net, val_loader)\n",
    "\n",
    "# noise removal\n",
    "m_probs[m_probs.view(m_probs.shape[0], -1).sum(-1) < 75*(SIZE/128.0)**2,...] = 0.0\n",
    "\n",
    "# search best threshold for this fold\n",
    "scores, best_thrs = [],[]\n",
    "dices = []\n",
    "thrs = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "m_probs = m_probs.cuda()\n",
    "truths = truths.cuda()\n",
    "\n",
    "with tqdm(total=len(thrs), file=sys.stdout) as pbar:\n",
    "    for th in thrs:\n",
    "        preds = (m_probs>th).long()\n",
    "        dices.append(dice_overall(preds, truths).mean())\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "dices = np.array(dices)    \n",
    "\n",
    "# save best\n",
    "scores.append(dices.max())\n",
    "best_thrs.append(thrs[dices.argmax()])\n",
    "\n",
    "print(dices, scores, best_thrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tgs)",
   "language": "python",
   "name": "tgs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
